# search.py

import os
import re
import base64
import json
import logging
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Tuple, Optional, Dict, Any
from dotenv import load_dotenv

import voyageai
from openai import OpenAI
from PIL import Image
from astrapy import DataAPIClient

# Importa utilit√°rios
from utils.metrics import ProcessingMetrics, measure_time
from utils.validation import validate_embedding
from utils.cache import SimpleCache

# Importa utils
from utils.metrics import ProcessingMetrics, measure_time
from utils.validation import validate_embedding

# Configura√ß√µes do sistema
LLM_MODEL = "gpt-4o" 
MAX_CANDIDATES = 5
MAX_TOKENS_RERANK = 512
MAX_TOKENS_ANSWER = 2048
MAX_TOKENS_QUERY_TRANSFORM = 150  # Reduzido para efici√™ncia
COLLECTION_NAME = "pdf_documents"

# Configura√ß√£o de logging espec√≠fica para o buscador
def setup_rag_logging():
    """Configura logging espec√≠fico para o RAG"""
    rag_logger = logging.getLogger(__name__)
    rag_logger.setLevel(logging.DEBUG)
    
    # Remove handlers existentes para evitar duplica√ß√£o
    for handler in rag_logger.handlers[:]:
        rag_logger.removeHandler(handler)
    
    # Formatter detalhado
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s"
    )
    
    # Handler para console
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)
    console_handler.setFormatter(formatter)
    rag_logger.addHandler(console_handler)
    
    # Handler para arquivo espec√≠fico do RAG
    file_handler = logging.FileHandler("rag_production_debug.log", encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    rag_logger.addHandler(file_handler)
    
    # N√£o propagar para o logger pai para evitar duplica√ß√£o
    rag_logger.propagate = False
    
    return rag_logger

logger = setup_rag_logging()

# Log inicial para confirmar que o sistema de logging est√° funcionando
logger.info("üöÄ [INIT] Sistema de logging do RAG inicializado")

def get_sao_paulo_time():
    """Retorna datetime atual no fuso hor√°rio de S√£o Paulo"""
    return datetime.now(ZoneInfo("America/Sao_Paulo"))

class ProductionQueryTransformer:
    """
    Transformador de queries otimizado
    - Menos chamadas LLM (economia de custos)
    - L√≥gica determin√≠stica quando poss√≠vel
    - Fallbacks robustos
    - Logging estruturado
    """
    
    def __init__(self, openai_client: OpenAI):
        self.openai_client = openai_client
        self.transformation_cache = {}  # Cache para transforma√ß√µes comuns
        self._init_patterns()
    
    def _init_patterns(self):
        """Inicializa padr√µes para classifica√ß√£o determin√≠stica"""
        self.greeting_patterns = {
            'simple': ['oi', 'ol√°', 'hello', 'hi', 'hey'],
            'formal': ['bom dia', 'boa tarde', 'boa noite', 'good morning'],
            'casual': ['opa', 'salve', 'e a√≠']
        }
        
        self.thank_patterns = [
            'obrigado', 'obrigada', 'thanks', 'thank you', 'valeu', 
            'brigado', 'grato', 'grata'
        ]
        
        self.document_terms = [
            'zep', 'graphiti', 'rag', 'temporal', 'knowledge graph',
            'grafo', 'arquitetura', 'paper', 'documento', 'artigo',
            'tabela', 'table', 'figura', 'figure', 'performance', 
            'resultado', 'metodologia', 'algorithm', 'invalida√ß√£o', 
            'mem√≥ria', 'embedding', 'vector', 'similarity'
        ]
        
        self.inquiry_keywords = [
            'explique', 'explain', 'como', 'how', 'o que', 'what',
            'quais', 'which', 'where', 'onde', 'quando', 'when',
            'por que', 'why', 'porque', 'me fale', 'tell me',
            'descreva', 'describe', 'mostre', 'show', 'qual',
            'quero saber', 'want to know', 'preciso entender',
            'pode explicar', 'can you explain'
        ]
        
        self.contextual_pronouns = [
            'isso', 'isto', 'aquilo', 'ele', 'ela', 'eles', 'elas',
            'this', 'that', 'these', 'those', 'it', 'they'
        ]
    
    def transform_query(self, chat_history: List[Dict[str, str]]) -> str:
        """
        Transforma√ß√£o principal com m√∫ltiplas estrat√©gias
        """
        import time
        transform_start = time.time()
        
        try:
            if not chat_history:
                logger.debug(f"[TRANSFORM] Hist√≥rico vazio, retornando 'Not applicable'")
                return "Not applicable"
            
            last_message = self._get_last_user_message(chat_history)
            if not last_message:
                logger.debug(f"[TRANSFORM] Nenhuma mensagem de usu√°rio encontrada")
                return "Not applicable"
            
            logger.debug(f"[TRANSFORM] Processando: '{last_message[:50]}...'")
            
            # Cache para transforma√ß√µes j√° feitas
            cache_key = self._create_cache_key(last_message, chat_history)
            if cache_key in self.transformation_cache:
                cached_result = self.transformation_cache[cache_key]
                logger.info(f"[TRANSFORM] üíæ Cache hit: '{cached_result[:50]}...'")
                return cached_result
            
            logger.debug(f"[TRANSFORM] Cache miss, processando...")
            
            # 1. Verifica√ß√µes determin√≠sticas (sem LLM)
            logger.debug(f"[TRANSFORM] Tentando classifica√ß√£o determin√≠stica...")
            deterministic_result = self._deterministic_classification(last_message, chat_history)
            if deterministic_result != "NEEDS_LLM":
                self.transformation_cache[cache_key] = deterministic_result
                transform_time = time.time() - transform_start
                logger.info(f"[TRANSFORM] ‚úÖ Determin√≠stica em {transform_time:.2f}s: '{deterministic_result[:50]}...'")
                return deterministic_result
            
            # 2. Transforma√ß√£o com LLM (apenas quando necess√°rio)
            logger.info(f"[TRANSFORM] ü§ñ Usando LLM para transforma√ß√£o complexa...")
            llm_start = time.time()
            llm_result = self._llm_transformation(last_message, chat_history)
            llm_time = time.time() - llm_start
            
            self.transformation_cache[cache_key] = llm_result
            
            total_time = time.time() - transform_start
            logger.info(f"[TRANSFORM] ‚úÖ LLM em {llm_time:.2f}s (total: {total_time:.2f}s): '{llm_result[:50]}...'")
            
            return llm_result
            
        except Exception as e:
            error_time = time.time() - transform_start
            logger.error(f"[TRANSFORM] ‚ùå Erro ap√≥s {error_time:.2f}s: {e}")
            # Fallback seguro
            fallback = self._safe_fallback(last_message if 'last_message' in locals() else "erro")
            logger.info(f"[TRANSFORM] üîÑ Fallback: '{fallback[:50]}...'")
            return fallback
    
    def _deterministic_classification(self, message: str, chat_history: List[Dict[str, str]]) -> str:
        """
        Classifica√ß√£o determin√≠stica sem LLM (mais r√°pida e barata)
        """
        message_lower = message.lower().strip()
        
        # 1. Sauda√ß√µes simples
        if self._is_simple_greeting(message_lower):
            return "Not applicable"
        
        # 2. Agradecimentos simples
        if self._is_simple_thanks(message_lower):
            return "Not applicable"
        
        # 3. Men√ß√µes diretas ao documento
        if self._mentions_document_directly(message_lower):
            return message  # J√° est√° bem formada
        
        # 4. Perguntas gerais sobre o documento
        if self._is_general_document_inquiry(message_lower):
            return f"Sobre o documento Zep: {message}"
        
        # 5. Refer√™ncias contextuais (pronomes)
        if self._has_contextual_references(message_lower) and self._has_document_context(chat_history):
            return f"Sobre o Zep: {message}"
        
        # 6. Perguntas com palavras-chave de consulta
        if self._has_inquiry_pattern(message_lower):
            return f"Sobre o documento Zep: {message}"
        
        # Se chegou aqui, precisa de LLM para contexto mais complexo
        return "NEEDS_LLM"
    
    def _is_simple_greeting(self, message: str) -> bool:
        """Detecta sauda√ß√µes simples"""
        words = message.split()
        
        # Sauda√ß√µes de 1-2 palavras
        if len(words) <= 2:
            for pattern_group in self.greeting_patterns.values():
                if all(word in pattern_group for word in words):
                    return True
        
        return False
    
    def _is_simple_thanks(self, message: str) -> bool:
        """Detecta agradecimentos simples"""
        return any(thank in message for thank in self.thank_patterns) and len(message.split()) <= 3
    
    def _mentions_document_directly(self, message: str) -> bool:
        """Verifica se menciona termos do documento diretamente"""
        return any(term in message for term in self.document_terms)
    
    def _is_general_document_inquiry(self, message: str) -> bool:
        """Detecta perguntas gerais que precisam de contexto do documento"""
        has_inquiry = any(keyword in message for keyword in self.inquiry_keywords)
        is_question = any(char in message for char in ['?', 'qual', 'como', 'o que'])
        
        return has_inquiry or is_question
    
    def _has_contextual_references(self, message: str) -> bool:
        """Detecta pronomes que referenciam contexto anterior"""
        return any(pronoun in message for pronoun in self.contextual_pronouns)
    
    def _has_document_context(self, chat_history: List[Dict[str, str]]) -> bool:
        """Verifica se h√° contexto do documento nas mensagens recentes"""
        recent_messages = chat_history[-6:]  # √öltimas 6 mensagens
        
        for msg in recent_messages:
            if msg.get('role') == 'assistant':
                content = msg.get('content', '').lower()
                if any(term in content for term in ['zep', 'graphiti', 'documento', 'p√°gina']):
                    return True
        
        return False
    
    def _has_inquiry_pattern(self, message: str) -> bool:
        """Detecta padr√µes de consulta"""
        return any(keyword in message for keyword in self.inquiry_keywords)
    
    def _llm_transformation(self, message: str, chat_history: List[Dict[str, str]]) -> str:
        """
        Transforma√ß√£o com LLM - usada apenas quando necess√°rio
        """
        try:
            # Contexto reduzido para economizar tokens
            recent_context = self._build_minimal_context(chat_history[-4:])
            
            prompt = f"""Transforme a mensagem em uma pergunta espec√≠fica sobre documentos acad√™micos.

REGRAS:
1. Se menciona "Zep", mantenha como est√°
2. Se √© pergunta geral, adicione "Sobre o Zep:"
3. Se referencia conversa anterior, combine contextos
4. Seja conciso e direto

CONTEXTO RECENTE:
{recent_context}

MENSAGEM: {message}

RESPONDA APENAS COM A PERGUNTA TRANSFORMADA:"""

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=MAX_TOKENS_QUERY_TRANSFORM,
                temperature=0.0
            )
            
            transformed = response.choices[0].message.content.strip()
            
            # Limpeza p√≥s-LLM
            transformed = self._clean_llm_output(transformed)
            
            logger.debug(f"LLM transform: '{message}' ‚Üí '{transformed}'")
            return transformed
            
        except Exception as e:
            logger.error(f"Erro na transforma√ß√£o LLM: {e}")
            return self._safe_fallback(message)
    
    def _build_minimal_context(self, recent_messages: List[Dict[str, str]]) -> str:
        """Constr√≥i contexto m√≠nimo para economizar tokens"""
        context_parts = []
        
        for msg in recent_messages:
            role = msg.get('role', '')
            content = msg.get('content', '')[:100]  # Limita a 100 chars
            
            if role in ['user', 'assistant']:
                context_parts.append(f"{role.title()}: {content}")
        
        return "\n".join(context_parts)
    
    def _clean_llm_output(self, output: str) -> str:
        """Limpa sa√≠da do LLM"""
        # Remove prefixos comuns
        prefixes = ['rag query:', 'query:', 'pergunta:', 'question:']
        
        for prefix in prefixes:
            if output.lower().startswith(prefix):
                output = output[len(prefix):].strip()
        
        # Remove aspas extras
        output = output.strip('"\'')
        
        return output
    
    def _safe_fallback(self, message: str) -> str:
        """Fallback seguro quando tudo mais falha"""
        if 'zep' in message.lower():
            return message
        else:
            return f"Sobre o documento Zep: {message}"
    
    def _create_cache_key(self, message: str, chat_history: List[Dict[str, str]]) -> str:
        """Cria chave de cache baseada na mensagem e contexto"""
        # Contexto simplificado para cache
        recent_topics = []
        for msg in chat_history[-3:]:
            content = msg.get('content', '').lower()
            if 'zep' in content:
                recent_topics.append('zep')
            if 'graphiti' in content:
                recent_topics.append('graphiti')
        
        context_key = '+'.join(set(recent_topics))
        return f"{message.lower()[:50]}||{context_key}"
    
    def _get_last_user_message(self, chat_history: List[Dict[str, str]]) -> str:
        """Pega √∫ltima mensagem do usu√°rio"""
        for msg in reversed(chat_history):
            if msg.get("role") == "user":
                return msg.get("content", "")
        return ""
    
    def needs_rag(self, transformed_query: str) -> bool:
        """Verifica se precisa fazer RAG"""
        return "not applicable" not in transformed_query.lower()
    
    def clean_query(self, transformed_query: str) -> str:
        """Limpa query final"""
        return transformed_query.strip()
    
    def get_cache_stats(self) -> Dict[str, int]:
        """Estat√≠sticas do cache para monitoramento"""
        return {
            "cache_size": len(self.transformation_cache),
            "cache_hits": getattr(self, '_cache_hits', 0),
            "llm_calls": getattr(self, '_llm_calls', 0)
        }

class ProductionConversationalRAG:
    """
    Sistema RAG conversacional otimizado
    """
    
    def __init__(self) -> None:
        """Inicializa com configura√ß√µes do sistema"""
        load_dotenv()

        # Inicializa sistema de m√©tricas
        self.metrics = ProcessingMetrics()
        
        # Inicializa cache para embeddings e respostas
        self.embedding_cache = SimpleCache(max_size=500, default_ttl=3600)  # 1 hora
        self.response_cache = SimpleCache(max_size=100, default_ttl=1800)   # 30 min

        # Valida√ß√£o de ambiente
        required_vars = [
            "VOYAGE_API_KEY", "OPENAI_API_KEY",
            "ASTRA_DB_API_ENDPOINT", "ASTRA_DB_APPLICATION_TOKEN"
        ]
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        if missing_vars:
            raise ValueError(f"Vari√°veis de ambiente ausentes: {missing_vars}")

        # Inicializa√ß√£o dos clientes
        voyageai.api_key = os.environ["VOYAGE_API_KEY"]
        self.voyage_client = voyageai.Client()
        self.openai_client = OpenAI()
        
        # Transformador otimizado
        self.query_transformer = ProductionQueryTransformer(self.openai_client)
        
        # Hist√≥rico da conversa
        self.chat_history: List[Dict[str, str]] = []

        # Conex√£o com Astra DB
        self._initialize_database()
        
        logger.info("Sistema RAG inicializado com sucesso")

    def _initialize_database(self):
        """Inicializa conex√£o com base de dados"""
        try:
            logger.info("Conectando ao Astra DB...")
            client = DataAPIClient()
            database = client.get_database(
                os.environ["ASTRA_DB_API_ENDPOINT"], 
                token=os.environ["ASTRA_DB_APPLICATION_TOKEN"]
            )
            self.collection = database.get_collection(COLLECTION_NAME)
            
            # Teste de conectividade
            list(self.collection.find({}, limit=1))
            logger.info(f"Conectado ao Astra DB - Collection '{COLLECTION_NAME}' acess√≠vel")
                
        except Exception as e:
            logger.error(f"Falha ao conectar Astra DB: {e}")
            raise

    def ask(self, user_message: str) -> str:
        """Interface conversacional principal otimizada"""
        logger.info(f"[ASK] === INICIANDO PROCESSAMENTO ===")
        logger.info(f"[ASK] Pergunta do usu√°rio: {user_message}")
        
        try:
            # Adiciona mensagem do usu√°rio ao hist√≥rico
            self.chat_history.append({"role": "user", "content": user_message})
            logger.debug(f"[ASK] Mensagem adicionada ao hist√≥rico. Total: {len(self.chat_history)} mensagens")
            
            # Transforma em query RAG com m√©tricas
            logger.info(f"[ASK] üîÑ ETAPA 1: Transformando query com IA...")
            
            with measure_time(self.metrics, "query_transformation"):
                transformed_query = self.query_transformer.transform_query(self.chat_history)
            
            logger.info(f"[ASK] ‚úÖ Query transformada: '{transformed_query}'")
            
            # Verifica se precisa fazer RAG
            needs_rag = self.query_transformer.needs_rag(transformed_query)
            logger.info(f"[ASK] ü§î ETAPA 2: Precisa fazer RAG? {needs_rag}")
            
            if not needs_rag:
                logger.info(f"[ASK] üí¨ Gerando resposta conversacional simples...")
                with measure_time(self.metrics, "non_rag_response"):
                    response = self._generate_non_rag_response(user_message)
                logger.info(f"[ASK] ‚úÖ Resposta simples gerada")
            else:
                # Limpa a query e faz RAG
                clean_query = self.query_transformer.clean_query(transformed_query)
                logger.info(f"[ASK] üßπ Query limpa: '{clean_query}'")
                logger.info(f"[ASK] üîç ETAPA 3: Iniciando busca RAG...")
                
                with measure_time(self.metrics, "rag_search"):
                    rag_result = self.search_and_answer(clean_query)
                
                if "error" in rag_result:
                    logger.warning(f"[ASK] ‚ùå RAG retornou erro: {rag_result['error']}")
                    response = f"Desculpe, n√£o consegui encontrar informa√ß√µes sobre isso. {rag_result['error']}"
                else:
                    logger.info(f"[ASK] ‚úÖ RAG completado")
                    logger.info(f"[ASK] üìä P√°ginas selecionadas: {rag_result.get('selected_pages_count', 0)}")
                    logger.info(f"[ASK] üìö Fonte: {rag_result.get('selected_pages', 'N/A')}")
                    response = rag_result["answer"]
            
            # Adiciona resposta ao hist√≥rico
            self.chat_history.append({"role": "assistant", "content": response})
            logger.debug(f"[ASK] Resposta adicionada ao hist√≥rico")
            
            # Limita hist√≥rico para controle de mem√≥ria
            if len(self.chat_history) > 20:
                old_len = len(self.chat_history)
                self.chat_history = self.chat_history[-16:]
                logger.debug(f"[ASK] Hist√≥rico limitado: {old_len} -> {len(self.chat_history)} mensagens")
            
            # Log de m√©tricas
            self.metrics.log_summary()
            logger.info(f"[ASK] ‚úÖ === PROCESSAMENTO COMPLETO ===")
            
            return response
            
        except Exception as e:
            logger.error(f"[ASK] ‚ùå Erro no processamento: {e}", exc_info=True)
            return "Desculpe, ocorreu um erro interno. Tente novamente."

    def _generate_non_rag_response(self, user_message: str) -> str:
        """Gera resposta para mensagens que n√£o precisam de RAG"""
        greetings = ["oi", "ol√°", "hello", "hi", "boa tarde", "bom dia", "boa noite"]
        
        if any(greeting in user_message.lower() for greeting in greetings):
            return "Ol√°! Sou seu assistente para consultas sobre documentos acad√™micos. Como posso ajudar voc√™ hoje?"
        
        thanks = ["obrigado", "obrigada", "thanks", "valeu"]
        if any(thank in user_message.lower() for thank in thanks):
            return "De nada! Fico feliz em ajudar. H√° mais alguma coisa que gostaria de saber?"
        
        return "Como posso ajudar voc√™ com consultas sobre os documentos? Fa√ßa uma pergunta espec√≠fica e eu buscarei as informa√ß√µes relevantes."

    # M√©todos de RAG originais (mantidos para compatibilidade)
    def get_query_embedding(self, query: str) -> List[float]:
        """Gera embedding para a consulta"""
        # Verifica cache primeiro
        cache_key = self.embedding_cache._create_key(query)
        cached_embedding = self.embedding_cache.get(cache_key)
        
        if cached_embedding is not None:
            logger.debug(f"Cache hit para embedding da query: {query[:50]}...")
            return cached_embedding
        
        try:
            res = self.voyage_client.multimodal_embed(
                inputs=[[query]],
                model="voyage-multimodal-3",
                input_type="query"
            )
            embedding = res.embeddings[0]
            
            # Valida embedding usando utils
            if not validate_embedding(embedding, 1024):
                raise ValueError("Embedding inv√°lido retornado pela API")
            
            # Armazena no cache
            self.embedding_cache.set(cache_key, embedding)
            logger.debug(f"Embedding cacheado para query: {query[:50]}...")
                
            return embedding
        except Exception as e:
            logger.error(f"Erro embedding consulta: {e}")
            raise

    @staticmethod
    def encode_image_to_base64(image_path: str) -> Optional[str]:
        """Converte imagem local em base64"""
        try:
            if not image_path or not os.path.exists(image_path):
                logger.warning(f"Imagem n√£o encontrada: {image_path}")
                return None
            with open(image_path, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            logger.error(f"Erro codificando {image_path}: {e}")
            return None

    def search_candidates(self, query_embedding: List[float], limit: int = MAX_CANDIDATES) -> List[dict]:
        """Busca candidatos no Astra DB"""
        try:
            logger.debug(f"[SEARCH] Buscando similaridade no Astra DB com limite de {limit}...")
            
            cursor = self.collection.find(
                {},
                sort={"$vector": query_embedding},
                limit=limit,
                include_similarity=True,
                projection={
                    "file_path": True,
                    "page_num": True,
                    "doc_source": True,
                    "markdown_text": True,
                    "_id": True
                }
            )
            
            candidates = []
            for doc in cursor:
                candidates.append({
                    "file_path": doc.get("file_path"),
                    "page_num": doc.get("page_num"),
                    "doc_source": doc.get("doc_source"),
                    "markdown_text": doc.get("markdown_text", ""),
                    "similarity_score": doc.get("$similarity", 0.0),
                })
            
            logger.info(f"[SEARCH] Busca retornou {len(candidates)} candidatos")
            return candidates
        except Exception as e:
            logger.error(f"Erro busca Astra DB: {e}")
            return []

    def verify_relevance(self, query: str, selected: List[dict]) -> bool:
        """Verifica relev√¢ncia do contexto selecionado"""
        if not selected:
            return False

        try:
            logger.debug(f"[RELEVANCE] Verificando relev√¢ncia com {len(selected)} p√°ginas selecionadas...")
            context_text = "\n\n".join(
                f"=== P√ÅGINA {c['page_num']} ===\n{c['markdown_text']}"
                for c in selected
            )

            prompt = (
                f"Analise o conte√∫do para responder: \"{query}\"\n\n"
                f"Conte√∫do:\n---\n{context_text}\n---\n\n"
                "O conte√∫do cont√©m resposta factual para a pergunta? "
                "Responda apenas 'Sim' ou 'N√£o'."
            )

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=5,
                temperature=0.0
            )
            
            verification_result = response.choices[0].message.content or ""
            logger.debug(f"Verifica√ß√£o de relev√¢ncia: '{verification_result}'")
            
            return "sim" in verification_result.lower()

        except Exception as e:
            logger.error(f"Erro na verifica√ß√£o de relev√¢ncia: {e}")
            return True  # Fallback conservador

    def rerank_with_gpt(self, query: str, candidates: List[dict]) -> Tuple[List[dict], str]:
        """Re-ranking com GPT-4"""
        if not candidates:
            return [], "Nenhuma p√°gina dispon√≠vel."

        if len(candidates) == 1:
            c = candidates[0]
            doc_name = os.path.basename(c["file_path"]).replace(".png", "")
            return [c], f"√önica p√°gina {doc_name}, p.{c['page_num']}."

        try:
            pages_info = ", ".join(
                f"{os.path.basename(c['file_path']).replace('.png','')} (p.{c['page_num']})"
                for c in candidates
            )
            
            prompt_head = (
                f"Pergunta: '{query}'.\n"
                f"P√°ginas ({len(candidates)}): {pages_info}.\n"
                "Selecione apenas a p√°gina mais relevante. "
                "M√°ximo 2 p√°ginas se absolutamente necess√°rio.\n\n"
                "Formato:\n"
                "P√°ginas_Selecionadas: [n¬∫] ou [n¬∫1, n¬∫2]\n"
                "Justificativa: ‚Ä¶"
            )
            content = [{"type": "text", "text": prompt_head}]

            for cand in candidates:
                b64 = self.encode_image_to_base64(cand["file_path"])
                if not b64:
                    continue
                    
                preview = cand["markdown_text"][:300]
                text_block = (
                    f"\n=== {os.path.basename(cand['file_path']).replace('.png','').upper()} "
                    f"- P√ÅGINA {cand['page_num']} ===\n"
                    f"Score: {cand['similarity_score']:.4f}\n"
                    f"Texto: {preview}{'‚Ä¶' if len(cand['markdown_text'])>300 else ''}\n"
                )
                content.append({"type": "text", "text": text_block})
                content.append({"type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{b64}"}})

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_RERANK,
                temperature=0.0
            )
            
            result = response.choices[0].message.content or ""
            logger.debug(f"Re-ranker: {result}")

            # Parse da resposta
            selected_nums: List[int] = []
            justification = "Justificativa ausente."
            
            for line in result.splitlines():
                if line.lower().startswith("p√°ginas_selecionadas"):
                    selected_nums = [int(n) for n in re.findall(r"\d+", line)]
                elif line.startswith("Justificativa:"):
                    justification = line.replace("Justificativa:", "").strip()

            chosen = [c for c in candidates if c["page_num"] in selected_nums]
            if chosen:
                return chosen, justification
                
            logger.warning("Re-ranker n√£o selecionou p√°ginas v√°lidas")
            return [candidates[0]], "Fallback: usando candidato mais similar."

        except Exception as e:
            logger.error(f"Erro re-ranking: {e}")
            return [candidates[0]], "Fallback: erro no re-ranker."

    def generate_conversational_answer(self, query: str, selected: List[dict]) -> str:
        """Gera resposta conversacional otimizada"""
        try:
            no_md = "N√ÉO use formata√ß√£o Markdown como **, _, #. Escreva texto corrido."
            
            if len(selected) == 1:
                c = selected[0]
                doc = os.path.basename(c["file_path"]).split("_page_")[0]
                
                prompt = (
                    f"Assistente especializado em documentos acad√™micos.\n"
                    f"Pergunta: {query}\n\n"
                    f"Use APENAS a p√°gina {c['page_num']} do documento '{doc}'.\n"
                    f"Texto da p√°gina:\n{c['markdown_text']}\n\n"
                    f"Instru√ß√µes: resposta clara e direta. Cite: documento '{doc}', p√°gina {c['page_num']}.\n"
                    f"{no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                b64 = self.encode_image_to_base64(c["file_path"])
                if b64:
                    content.append({"type": "image_url",
                                    "image_url": {"url": f"data:image/png;base64,{b64}"}})

            else:
                pages_str = " e ".join(
                    f"{os.path.basename(c['file_path']).split('_page_')[0]} p.{c['page_num']}"
                    for c in selected
                )
                combined_text = "\n\n".join(
                    f"=== P√ÅGINA {c['page_num']} ===\n{c['markdown_text']}"
                    for c in selected
                )
                
                prompt = (
                    f"Pergunta: {query}\n\n"
                    f"Use p√°ginas: {pages_str}\n"
                    f"{combined_text}\n\n"
                    f"Integre informa√ß√µes. Cite fontes. {no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                for c in selected:
                    b64 = self.encode_image_to_base64(c["file_path"])
                    if b64:
                        content.append({"type": "text", "text": f"\n--- P√ÅGINA {c['page_num']} ---"})
                        content.append({"type": "image_url",
                                        "image_url": {"url": f"data:image/png;base64,{b64}"}})

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_ANSWER,
                temperature=0.2
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Erro gerando resposta: {e}")
            return f"Erro ao processar resposta: {e}"

    def search_and_answer(self, query: str) -> dict:
        """Pipeline completo RAG"""
        import time
        pipeline_start = time.time()
        
        logger.info(f"[RAG] === PIPELINE RAG INICIADO ===")
        logger.info(f"[RAG] Query: '{query}'")
        
        # ETAPA 1: Gerar embedding
        logger.info(f"[RAG] üßÆ ETAPA 1: Gerando embedding da query...")
        embedding_start = time.time()
        
        try:
            embedding = self.get_query_embedding(query)
            embedding_time = time.time() - embedding_start
            logger.info(f"[RAG] ‚úÖ Embedding gerado em {embedding_time:.2f}s (dimens√£o: {len(embedding)})")
        except Exception as e:
            logger.error(f"[RAG] ‚ùå Embedding falhou: {e}")
            return {"error": f"Embedding falhou: {e}"}

        # ETAPA 2: Buscar candidatos
        logger.info(f"[RAG] üîç ETAPA 2: Buscando candidatos no Astra DB...")
        search_start = time.time()
        
        candidates = self.search_candidates(embedding)
        search_time = time.time() - search_start
        
        if not candidates:
            logger.warning(f"[RAG] ‚ùå Nenhum candidato encontrado em {search_time:.2f}s")
            return {"error": "Nenhuma p√°gina relevante encontrada."}
        
        logger.info(f"[RAG] ‚úÖ {len(candidates)} candidatos encontrados em {search_time:.2f}s")
        
        # Log dos candidatos com scores
        for i, cand in enumerate(candidates[:3]):  # Mostra top 3
            logger.debug(f"[RAG] Candidato {i+1}: P√°gina {cand['page_num']} (score: {cand['similarity_score']:.3f})")

        # ETAPA 3: Re-ranking
        logger.info(f"[RAG] üéØ ETAPA 3: Re-rankeando candidatos com GPT...")
        rerank_start = time.time()
        
        selected, justification = self.rerank_with_gpt(query, candidates)
        rerank_time = time.time() - rerank_start
        
        if not selected:
            logger.error(f"[RAG] ‚ùå Re-ranking falhou em {rerank_time:.2f}s")
            return {"error": "Re-ranking falhou."}
        
        logger.info(f"[RAG] ‚úÖ Re-ranking completo em {rerank_time:.2f}s")
        logger.info(f"[RAG] üìã {len(selected)} p√°ginas selecionadas")
        logger.debug(f"[RAG] Justificativa: {justification}")

        # ETAPA 4: Verificar relev√¢ncia
        logger.info(f"[RAG] ‚úÖ ETAPA 4: Verificando relev√¢ncia...")
        relevance_start = time.time()
        
        is_relevant = self.verify_relevance(query, selected)
        relevance_time = time.time() - relevance_start
        
        if not is_relevant:
            logger.warning(f"[RAG] ‚ùå Verifica√ß√£o de relev√¢ncia falhou em {relevance_time:.2f}s")
            return {
                "error": "A informa√ß√£o solicitada n√£o foi encontrada de forma expl√≠cita no documento."
            }
        
        logger.info(f"[RAG] ‚úÖ Relev√¢ncia confirmada em {relevance_time:.2f}s")

        # ETAPA 5: Gerar resposta
        logger.info(f"[RAG] üí¨ ETAPA 5: Gerando resposta final...")
        answer_start = time.time()
        
        answer = self.generate_conversational_answer(query, selected)
        answer_time = time.time() - answer_start
        
        logger.info(f"[RAG] ‚úÖ Resposta gerada em {answer_time:.2f}s")
        
        total_pipeline_time = time.time() - pipeline_start
        logger.info(f"[RAG] üèÅ === PIPELINE COMPLETO em {total_pipeline_time:.2f}s ===")

        # Prepara detalhes da resposta
        sel_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in selected
        ]
        
        all_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in candidates
        ]
        
        sel_str = " + ".join(
            f"{p['document']} (p.{p['page_number']})" for p in sel_details
        )

        return {
            "query": query,
            "selected_pages": sel_str,
            "selected_pages_details": sel_details,
            "selected_pages_count": len(selected),
            "justification": justification,
            "answer": answer,
            "total_candidates": len(candidates),
            "all_candidates": all_details,
        }

    def extract_structured_data(self, template: dict, document_filter: Optional[str] = None) -> dict:
        """Extra√ß√£o de dados estruturados"""
        try:
            # Busca p√°ginas relevantes
            if document_filter:
                pages_cursor = self.collection.find(
                    {"doc_source": document_filter},
                    limit=10,
                    projection={
                        "file_path": True,
                        "page_num": True,
                        "doc_source": True,
                        "markdown_text": True
                    }
                )
            else:
                pages_cursor = self.collection.find(
                    {},
                    limit=10,
                    projection={
                        "file_path": True,
                        "page_num": True,
                        "doc_source": True,
                        "markdown_text": True
                    }
                )
            
            pages = list(pages_cursor)
            if not pages:
                return {"error": "Nenhuma p√°gina encontrada"}
            
            # Prepara prompt de extra√ß√£o
            template_str = json.dumps(template, indent=2)
            
            content = [{
                "type": "text",
                "text": f"""
Extraia dados estruturados seguindo este template: {template_str}

Se informa√ß√£o n√£o dispon√≠vel, deixe em branco.
Responda APENAS com JSON v√°lido.

DOCUMENTOS:"""
            }]
            
            # Adiciona p√°ginas (limitado para n√£o exceder tokens)
            for page in pages[:5]:
                doc_name = page.get("doc_source", "documento")
                page_num = page.get("page_num", 0)
                content_text = page.get("markdown_text", "")[:500]
                
                content.append({
                    "type": "text",
                    "text": f"\n=== {doc_name.upper()} - P√ÅGINA {page_num} ===\n{content_text}\n"
                })
                
                # Adiciona imagem se dispon√≠vel
                img_b64 = self.encode_image_to_base64(page.get("file_path"))
                if img_b64:
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{img_b64}"}
                    })
            
            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            extracted_data = json.loads(response.choices[0].message.content)
            
            return {
                "status": "success",
                "data": extracted_data,
                "pages_analyzed": len(pages)
            }
            
        except Exception as e:
            logger.error(f"Erro na extra√ß√£o de dados: {e}")
            return {
                "status": "error",
                "message": f"Erro na extra√ß√£o: {e}"
            }

    def clear_history(self):
        """Limpa hist√≥rico da conversa"""
        self.chat_history = []
        logger.info("Hist√≥rico de conversa limpo")

    def get_chat_history(self) -> List[Dict[str, str]]:
        """Retorna hist√≥rico atual"""
        return self.chat_history.copy()

    def get_system_stats(self) -> Dict[str, Any]:
        """Estat√≠sticas do sistema para monitoramento"""
        stats = {
            "chat_history_length": len(self.chat_history),
            "transformer_stats": self.query_transformer.get_cache_stats(),
            "system_health": "operational"
        }
        
        try:
            # Teste de conectividade
            list(self.collection.find({}, limit=1))
            stats["database_status"] = "connected"
        except:
            stats["database_status"] = "error"
            stats["system_health"] = "degraded"
        
        return stats

# Wrapper para compatibilidade com c√≥digo existente
class ConversationalMultimodalRAG(ProductionConversationalRAG):
    """Alias para compatibilidade com c√≥digo existente"""
    pass

# Classe simples para interface externa
class SimpleRAG:
    """Interface simplificada para uso externo"""
    
    def __init__(self):
        self.rag = ProductionConversationalRAG()
    
    def search(self, query: str) -> str:
        """Busca simples"""
        return self.rag.ask(query)
    
    def extract(self, template: dict, document: str = None) -> dict:
        """Extrai dados estruturados"""
        return self.rag.extract_structured_data(template, document)
    
    def clear_chat(self):
        """Limpa hist√≥rico"""
        self.rag.clear_history()

# Interface CLI otimizada
def main() -> None:
    """Interface CLI com tratamento de erros robusto"""
    try:
        rag = ProductionConversationalRAG()
        
        print("üöÄ SISTEMA RAG CONVERSACIONAL üöÄ")
        print("=" * 70)
        print("‚ú® Otimiza√ß√µes ativas:")
        print("  ‚Ä¢ Query transformer inteligente")
        print("  ‚Ä¢ Cache de transforma√ß√µes")
        print("  ‚Ä¢ Logging estruturado")
        print("  ‚Ä¢ Fallbacks robustos")
        print("=" * 70)

        # CORRE√á√ÉO APLICADA AQUI:
        # A chamada autom√°tica foi removida e substitu√≠da por uma sauda√ß√£o est√°tica.
        print("ü§ñ Assistente: Ol√°! Sou seu assistente de documentos. Fa√ßa sua pergunta ou digite /help para ver os comandos.")
        print()

        while True:
            try:
                user_input = input("üí¨ Voc√™: ").strip()
                
                if user_input.lower() in {"sair", "exit", "quit", "/quit"}:
                    print("üëã At√© logo!")
                    break
                
                if not user_input:
                    continue
                
                # Comandos especiais
                if user_input.startswith("/"):
                    if user_input == "/help":
                        print_production_help()
                    elif user_input == "/clear":
                        rag.clear_history()
                        print("üßπ Hist√≥rico limpo!")
                    elif user_input == "/stats":
                        stats = rag.get_system_stats()
                        print("üìä Estat√≠sticas do sistema:")
                        for key, value in stats.items():
                            print(f"  {key}: {value}")
                    elif user_input.startswith("/extract"):
                        handle_production_extract_command(rag, user_input)
                    else:
                        print("‚ùì Comando n√£o reconhecido. Digite /help")
                    continue

                # Resposta normal
                print("\nü§ñ Assistente: ", end="")
                try:
                    response = rag.ask(user_input)
                    print(response)
                except Exception as e:
                    logger.error(f"Erro no processamento: {e}")
                    print("‚ùå Erro tempor√°rio. Tente novamente.")
                print()

            except KeyboardInterrupt:
                print("\nüëã At√© logo!")
                break
            except Exception as e:
                logger.error(f"Erro na interface: {e}")
                print("‚ùå Erro na interface. Continuando...")

    except Exception as e:
        logger.critical(f"Erro fatal: {e}")
        print(f"‚ùå Erro fatal na inicializa√ß√£o: {e}")
        print("Verifique:")
        print("1. Arquivo .env com chaves corretas")
        print("2. Conex√£o com Astra DB")
        print("3. Documentos indexados")

def print_production_help():
    """Ajuda do sistema"""
    print("""
üìö COMANDOS:
‚Ä¢ /help     - Esta ajuda
‚Ä¢ /clear    - Limpa hist√≥rico
‚Ä¢ /stats    - Estat√≠sticas do sistema
‚Ä¢ /extract  - Extra√ß√£o de dados
  Exemplo: /extract {\"title\": \"\", \"authors\": []}

üí° RECURSOS:
‚Ä¢ Cache de transforma√ß√µes (economia de custos)
‚Ä¢ Fallbacks autom√°ticos (maior robustez)
‚Ä¢ Logging estruturado (monitoramento)
‚Ä¢ Valida√ß√£o de ambiente (seguran√ßa)

üîç TIPOS DE CONSULTA OTIMIZADOS:
‚Ä¢ Perguntas diretas sobre o Zep
‚Ä¢ Refer√™ncias contextuais ("como funciona isso?")
‚Ä¢ Consultas t√©cnicas espec√≠ficas
‚Ä¢ Seguimento de conversas anteriores
""")

def handle_production_extract_command(rag, command):
    """Manipula extra√ß√£o de dados do sistema"""
    try:
        if len(command.split(" ", 1)) < 2:
            print("üí° Uso: /extract {\"campo\": \"valor\"}")
            print("üìù Exemplo: /extract {\"title\": \"\", \"methodology\": \"\"}")
            return
        
        template_str = command.split(" ", 1)[1]
        template = json.loads(template_str)
        
        print("üîç Extraindo dados (produ√ß√£o)...")
        result = rag.extract_structured_data(template)
        
        if result.get("status") == "success":
            print("‚úÖ Extra√ß√£o bem-sucedida:")
            print(json.dumps(result["data"], indent=2, ensure_ascii=False))
            print(f"üìä P√°ginas analisadas: {result['pages_analyzed']}")
        else:
            print(f"‚ùå Erro: {result.get('message')}")
            
    except json.JSONDecodeError:
        print("‚ùå JSON inv√°lido. Use aspas duplas!")
    except Exception as e:
        logger.error(f"Erro na extra√ß√£o: {e}")
        print(f"‚ùå Erro: {e}")

# Para monitoramento do sistema
def health_check() -> Dict[str, str]:
    """Health check para monitoramento"""
    try:
        rag = ProductionConversationalRAG()
        stats = rag.get_system_stats()
        
        return {
            "status": "healthy" if stats["system_health"] == "operational" else "degraded",
            "database": stats["database_status"],
            "cache_size": str(stats["transformer_stats"]["cache_size"]),
            "timestamp": get_sao_paulo_time().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "timestamp": get_sao_paulo_time().isoformat()
        }

# Aliases para compatibilidade
MultimodalRagSearcher = ProductionConversationalRAG  # Para avaliador
EnhancedMultimodalRagSearcher = ProductionConversationalRAG  # Para sistemas melhorados

__all__ = [
    'ProductionConversationalRAG', 
    'ConversationalMultimodalRAG',
    'SimpleRAG', 
    'MultimodalRagSearcher',
    'health_check'
]

if __name__ == "__main__":
    main()