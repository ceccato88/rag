# buscador_otimizado.py

import os
import re
import base64
import logging
from typing import List, Tuple, Optional
from dotenv import load_dotenv

import voyageai
from openai import OpenAI
from PIL import Image
from astrapy import DataAPIClient

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ConfiguraÃ§Ãµes Otimizadas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM_MODEL = "gpt-4.1"              # GPT-4.1 (mais inteligente)
MAX_INITIAL_FETCH = 8               # Fase 1: Busca ampla (era 5)
MAX_FINAL_SELECTION = 2             # Fase 2: SeleÃ§Ã£o final
MAX_TOKENS_RERANK = 512            
MAX_TOKENS_ANSWER = 2048           
COLLECTION_NAME = "pdf_documents"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Classe Otimizada â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class OptimizedMultimodalRagSearcher:
    def __init__(self) -> None:
        """Inicializa com GPT-4.1 e lÃ³gica de duas fases."""
        load_dotenv()

        required = [
            "VOYAGE_API_KEY", "OPENAI_API_KEY",
            "ASTRA_DB_API_ENDPOINT", "ASTRA_DB_APPLICATION_TOKEN"
        ]
        for k in required:
            if not os.getenv(k):
                raise ValueError(f"Chave {k} nÃ£o encontrada em .env")

        voyageai.api_key = os.environ["VOYAGE_API_KEY"]
        self.voyage_client = voyageai.Client()
        self.openai_client = OpenAI()

        try:
            logger.info("Conectando ao Astra DBâ€¦")
            client = DataAPIClient()
            database = client.get_database(
                os.environ["ASTRA_DB_API_ENDPOINT"], 
                token=os.environ["ASTRA_DB_APPLICATION_TOKEN"]
            )
            self.collection = database.get_collection(COLLECTION_NAME)
            
            try:
                list(self.collection.find({}, limit=1))
                logger.info("âœ… Conectado ao Astra DB - Collection '%s' acessÃ­vel", COLLECTION_NAME)
            except Exception:
                logger.error("âŒ Collection '%s' nÃ£o encontrada ou inacessÃ­vel", COLLECTION_NAME)
                raise
                
        except Exception as e:
            logger.error("Falha ao conectar Astra DB: %s", e)
            raise

        logger.info("ğŸš€ Sistema RAG Otimizado pronto com GPT-4.1!")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Embedding da consulta â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_query_embedding(self, query: str) -> List[float]:
        """Gera embedding de texto para a consulta."""
        try:
            res = self.voyage_client.multimodal_embed(
                inputs=[[query]],
                model="voyage-multimodal-3",
                input_type="query"
            )
            return res.embeddings[0]
        except Exception as e:
            logger.error("Erro embedding consulta: %s", e)
            raise

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utilidades de imagem â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def encode_image_to_base64(image_path: str) -> Optional[str]:
        """Converte imagem local em base64."""
        try:
            if not image_path or not os.path.exists(image_path):
                logger.warning("Imagem nÃ£o encontrada: %s", image_path)
                return None
            with open(image_path, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            logger.error("Erro codificando %s: %s", image_path, e)
            return None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ FASE 1: Busca Ampla â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def phase1_broad_search(self, query_embedding: List[float]) -> List[dict]:
        """
        FASE 1: Busca ampla - "Cast a Wide Net"
        Busca mais candidatos para nÃ£o perder documentos relevantes.
        """
        try:
            logger.info("ğŸŒ FASE 1: Busca ampla (top-%d candidatos)...", MAX_INITIAL_FETCH)
            
            cursor = self.collection.find(
                {},
                sort={"$vector": query_embedding},
                limit=MAX_INITIAL_FETCH,  # Busca mais candidatos
                include_similarity=True,
                projection={
                    "file_path": True,
                    "page_num": True,
                    "doc_source": True,
                    "markdown_text": True,
                    "_id": True
                }
            )
            
            candidates = []
            for doc in cursor:
                candidates.append({
                    "file_path": doc.get("file_path"),
                    "page_num": doc.get("page_num"),
                    "doc_source": doc.get("doc_source"),
                    "markdown_text": doc.get("markdown_text", ""),
                    "similarity_score": doc.get("$similarity", 0.0),
                })
            
            logger.info("ğŸ“Š Fase 1 retornou %d candidatos", len(candidates))
            
            # Log dos scores para anÃ¡lise
            for i, c in enumerate(candidates):
                logger.info("   Candidato %d: PÃ¡gina %d, Score=%.4f", 
                           i+1, c['page_num'], c['similarity_score'])
            
            return candidates
        except Exception as e:
            logger.error("Erro na Fase 1: %s", e)
            return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ FASE 2: SeleÃ§Ã£o Precisa com GPT-4.1 â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def phase2_precise_reranking(self, query: str, candidates: List[dict]) -> Tuple[List[dict], str]:
        """
        FASE 2: SeleÃ§Ã£o precisa - "Be Selective"
        Usa GPT-4.1 para escolher os melhores candidatos da Fase 1.
        """
        if not candidates:
            return [], "Nenhum candidato da Fase 1."

        if len(candidates) == 1:
            c = candidates[0]
            doc_name = os.path.basename(c["file_path"]).replace(".png", "")
            return [c], f"Ãšnico candidato {doc_name}, p.{c['page_num']}."

        try:
            logger.info("ğŸ¯ FASE 2: SeleÃ§Ã£o precisa com GPT-4.1 (%d â†’ %d)...", 
                       len(candidates), MAX_FINAL_SELECTION)
            
            pages_info = ", ".join(
                f"{os.path.basename(c['file_path']).replace('.png','')} (p.{c['page_num']}, score={c['similarity_score']:.3f})"
                for c in candidates
            )
            
            # Prompt otimizado para GPT-4.1
            prompt_head = (
                f"VocÃª Ã© um assistente especialista usando GPT-4.1 para seleÃ§Ã£o precisa de documentos.\n\n"
                f"PERGUNTA: '{query}'\n\n"
                f"CANDIDATOS da Fase 1 ({len(candidates)} pÃ¡ginas): {pages_info}\n\n"
                f"TAREFA:\n"
                f"Analise cada pÃ¡gina e selecione APENAS as mais relevantes que contenham informaÃ§Ã£o especÃ­fica para responder Ã  pergunta.\n"
                f"- Priorize pÃ¡ginas com informaÃ§Ã£o direta e factual\n"
                f"- MÃ¡ximo {MAX_FINAL_SELECTION} pÃ¡ginas\n"
                f"- Se uma pÃ¡gina jÃ¡ responde completamente, nÃ£o selecione outras\n\n"
                f"FORMATO DE RESPOSTA:\n"
                f"PÃ¡ginas_Selecionadas: [nÂº] ou [nÂº1, nÂº2]\n"
                f"Justificativa: Explique brevemente por que essas pÃ¡ginas sÃ£o as melhores\n"
                f"Confidence: Alta/MÃ©dia/Baixa"
            )
            
            content = [{"type": "text", "text": prompt_head}]

            # Adicionar contexto visual de cada candidato
            for cand in candidates:
                b64 = self.encode_image_to_base64(cand["file_path"])
                if not b64:
                    continue
                    
                preview = cand["markdown_text"][:400]  # Mais contexto
                text_block = (
                    f"\n=== PÃGINA {cand['page_num']} ===\n"
                    f"Documento: {os.path.basename(cand['file_path']).replace('.png','').upper()}\n"
                    f"Similarity Score: {cand['similarity_score']:.4f}\n"
                    f"ConteÃºdo: {preview}{'â€¦' if len(cand['markdown_text'])>400 else ''}\n"
                )
                content.append({"type": "text", "text": text_block})
                content.append({"type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{b64}"}})

            # Usar GPT-4.1 para seleÃ§Ã£o precisa
            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,  # GPT-4.1
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_RERANK,
                temperature=0.0  # DeterminÃ­stico para consistÃªncia
            )
            
            result = response.choices[0].message.content or ""
            logger.info("ğŸ“ Resposta GPT-4.1 Fase 2: %s", result)

            # Parser da resposta
            selected_nums: List[int] = []
            justification = "Justificativa ausente."
            confidence = "MÃ©dia"
            
            for line in result.splitlines():
                line = line.strip()
                if line.lower().startswith("pÃ¡ginas_selecionadas"):
                    selected_nums = [int(n) for n in re.findall(r"\d+", line)]
                elif line.startswith("Justificativa:"):
                    justification = line.replace("Justificativa:", "").strip()
                elif line.startswith("Confidence:"):
                    confidence = line.replace("Confidence:", "").strip()

            # Mapear nÃºmeros selecionados para candidatos
            chosen = [c for c in candidates if c["page_num"] in selected_nums]
            
            if chosen:
                logger.info("âœ… Fase 2 selecionou %d pÃ¡ginas: %s (Confidence: %s)", 
                           len(chosen), [c['page_num'] for c in chosen], confidence)
                return chosen, f"{justification} (Confidence: {confidence})"
            else:
                logger.warning("âš ï¸ GPT-4.1 nÃ£o selecionou pÃ¡ginas vÃ¡lidas; usando a mais similar.")
                return [candidates[0]], "Fallback: usando candidato com maior similarity."

        except Exception as e:
            logger.error("Erro na Fase 2: %s", e)
            return [candidates[0]], "Fallback: erro na seleÃ§Ã£o com GPT-4.1."

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ VerificaÃ§Ã£o de RelevÃ¢ncia â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def verify_relevance(self, query: str, selected: List[dict]) -> bool:
        """Verifica se a resposta estÃ¡ de fato no contexto selecionado."""
        if not selected:
            return False

        try:
            logger.info("ğŸ” VerificaÃ§Ã£o de relevÃ¢ncia...")
            context_text = "\n\n".join(
                f"=== PÃGINA {c['page_num']} ===\n{c['markdown_text']}"
                for c in selected
            )

            prompt = (
                f"Analise o conteÃºdo para responder: \"{query}\"\n\n"
                f"ConteÃºdo selecionado:\n---\n{context_text}\n---\n\n"
                f"O conteÃºdo contÃ©m informaÃ§Ã£o factual e especÃ­fica para responder Ã  pergunta? "
                f"Considere apenas respostas diretas e explÃ­citas.\n"
                f"Responda apenas: 'Sim' ou 'NÃ£o'"
            )

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,  # GPT-4.1 para verificaÃ§Ã£o tambÃ©m
                messages=[{"role": "user", "content": prompt}],
                max_tokens=5,
                temperature=0.0
            )
            
            verification_result = response.choices[0].message.content or ""
            logger.info("ğŸ“‹ VerificaÃ§Ã£o de relevÃ¢ncia: '%s'", verification_result)
            
            return "sim" in verification_result.lower()

        except Exception as e:
            logger.error("Erro na verificaÃ§Ã£o de relevÃ¢ncia: %s", e)
            return True  # Fallback conservador

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ GeraÃ§Ã£o da resposta final com GPT-4.1 â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def generate_final_answer(self, query: str, selected: List[dict]) -> str:
        """ConstrÃ³i resposta final usando GPT-4.1."""
        try:
            logger.info("ğŸ“ Gerando resposta final com GPT-4.1...")
            
            no_md = "- NÃƒO use formataÃ§Ã£o Markdown como **, _, #. Escreva texto corrido."
            
            if len(selected) == 1:
                c = selected[0]
                doc = os.path.basename(c["file_path"]).split("_page_")[0]
                prompt = (
                    f"VocÃª Ã© um assistente especializado usando GPT-4.1 para anÃ¡lise de documentos acadÃªmicos.\n\n"
                    f"PERGUNTA: {query}\n\n"
                    f"DOCUMENTO: Use APENAS a pÃ¡gina {c['page_num']} do documento '{doc}' abaixo.\n\n"
                    f"CONTEÃšDO DA PÃGINA:\n{c['markdown_text']}\n\n"
                    f"INSTRUÃ‡Ã•ES:\n"
                    f"- Responda com base exclusivamente no conteÃºdo fornecido\n"
                    f"- Se a resposta estiver presente, explique de forma clara e completa\n"
                    f"- Se nÃ£o estiver, informe que a informaÃ§Ã£o especÃ­fica nÃ£o estÃ¡ disponÃ­vel\n"
                    f"- Mencione que a resposta vem do documento '{doc}', pÃ¡gina {c['page_num']}\n"
                    f"- Use linguagem clara e precisa\n"
                    f"{no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                b64 = self.encode_image_to_base64(c["file_path"])
                if b64:
                    content.append({"type": "image_url",
                                    "image_url": {"url": f"data:image/png;base64,{b64}", "detail": "high"}})

            else:
                # MÃºltiplas pÃ¡ginas
                pages_str = " e ".join(
                    f"{os.path.basename(c['file_path']).split('_page_')[0]} p.{c['page_num']}"
                    for c in selected
                )
                combined_text = "\n\n".join(
                    f"=== PÃGINA {c['page_num']} ===\n{c['markdown_text']}"
                    for c in selected
                )
                prompt = (
                    f"VocÃª Ã© um assistente especializado usando GPT-4.1.\n\n"
                    f"PERGUNTA: {query}\n\n"
                    f"DOCUMENTOS: Use APENAS as pÃ¡ginas {pages_str} abaixo.\n\n"
                    f"CONTEÃšDO COMBINADO:\n{combined_text}\n\n"
                    f"INSTRUÃ‡Ã•ES:\n"
                    f"- Integre informaÃ§Ãµes de todas as pÃ¡ginas relevantes\n"
                    f"- Seja claro sobre qual pÃ¡gina contÃ©m cada informaÃ§Ã£o\n"
                    f"- Se alguma informaÃ§Ã£o estiver ausente, mencione explicitamente\n"
                    f"- Cite as pÃ¡ginas utilizadas na resposta\n"
                    f"{no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                for c in selected:
                    b64 = self.encode_image_to_base64(c["file_path"])
                    if b64:
                        content.append({"type": "text", "text": f"\n--- IMAGEM PÃGINA {c['page_num']} ---"})
                        content.append({"type": "image_url",
                                        "image_url": {"url": f"data:image/png;base64,{b64}", "detail": "high"}})

            # GPT-4.1 para resposta final
            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_ANSWER,
                temperature=0.1
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error("Erro gerando resposta final: %s", e, exc_info=True)
            return f"Erro ao gerar resposta com GPT-4.1: {e}"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pipeline Completo de Duas Fases â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def search_and_answer(self, query: str) -> dict:
        """Pipeline otimizado com duas fases: Busca Ampla â†’ SeleÃ§Ã£o Precisa."""
        logger.info("ğŸš€ Iniciando pipeline otimizado de duas fases...")
        logger.info("ğŸ“ Consulta: '%s'", query)
        
        try:
            embedding = self.get_query_embedding(query)
        except Exception as e:
            return {"error": f"Embedding falhou: {e}"}

        # FASE 1: Busca ampla
        candidates = self.phase1_broad_search(embedding)
        if not candidates:
            return {"error": "Fase 1: Nenhuma pÃ¡gina relevante encontrada."}

        # FASE 2: SeleÃ§Ã£o precisa com GPT-4.1
        selected, justification = self.phase2_precise_reranking(query, candidates)
        if not selected:
            return {"error": "Fase 2: Falha na seleÃ§Ã£o precisa."}

        # VerificaÃ§Ã£o de relevÃ¢ncia
        if not self.verify_relevance(query, selected):
            logger.warning(
                "âŒ VerificaÃ§Ã£o de relevÃ¢ncia indicou que a resposta nÃ£o estÃ¡ no contexto selecionado. "
                "Interrompendo para evitar resposta incorreta."
            )
            return {
                "error": "A informaÃ§Ã£o solicitada nÃ£o foi encontrada de forma explÃ­cita no documento."
            }

        # GeraÃ§Ã£o da resposta final
        logger.info("ğŸ“ Gerando resposta final...")
        answer = self.generate_final_answer(query, selected)

        # Preparar detalhes da resposta
        sel_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in selected
        ]
        
        all_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in candidates
        ]
        
        sel_str = " + ".join(
            f"{p['document']} (p.{p['page_number']})" for p in sel_details
        )

        return {
            "query": query,
            "selected_pages": sel_str,
            "selected_pages_details": sel_details,
            "selected_pages_count": len(selected),
            "justification": justification,
            "answer": answer,
            "total_candidates": len(candidates),
            "all_candidates": all_details,
            "pipeline_info": {
                "phase1_candidates": len(candidates),
                "phase2_selected": len(selected),
                "model_used": LLM_MODEL,
                "optimization": "Two-Phase Retrieval"
            }
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Interface CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    try:
        searcher = OptimizedMultimodalRagSearcher()
        print("ğŸš€ RAG OTIMIZADO - Two-Phase Retrieval com GPT-4.1 ğŸš€")
        print("=" * 70)
        print("ğŸ“Š ConfiguraÃ§Ã£o: Fase 1 (top-8) â†’ Fase 2 (top-2) â†’ GPT-4.1")
        print("=" * 70)

        while True:
            user_q = input("ğŸ’¬ Sua pergunta: ").strip()
            if user_q.lower() in {"sair", "exit", "quit"}:
                print("ğŸ‘‹ AtÃ© logo!")
                break
            if not user_q:
                continue

            print("\n" + "â”€" * 70 + "\nğŸ” Processando com pipeline de duas fases...")
            result = searcher.search_and_answer(user_q)

            if "error" in result:
                print("âŒ", result["error"])
                continue

            print(f"\nğŸ“„ PÃ¡ginas selecionadas: {result['selected_pages']}")
            print(f"ğŸ¤– Justificativa: {result['justification']}")
            
            pipeline_info = result.get('pipeline_info', {})
            print(f"âš™ï¸ Pipeline: {pipeline_info.get('phase1_candidates', 0)} â†’ {pipeline_info.get('phase2_selected', 0)} pÃ¡ginas")
            
            print("\nğŸ“ RESPOSTA:\n" + "â•" * 70)
            print(result["answer"])
            print("â•" * 70 + "\n")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ AtÃ© logo!")
    except Exception as e:
        logger.critical("Erro fatal: %s", e, exc_info=True)
        print("âŒ Erro fatal:", e)

__all__ = ['OptimizedMultimodalRagSearcher']

if __name__ == "__main__":
    main()