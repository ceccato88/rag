# Estrutura de Testes do Projeto RAG

Este documento descreve a estrutura de testes implementada para o projeto RAG (Retrieval-Augmented Generation).

## ğŸ“ Estrutura de DiretÃ³rios

```
tests/
â”œâ”€â”€ conftest.py                     # ConfiguraÃ§Ã£o global e fixtures compartilhadas
â”œâ”€â”€ run_tests.py                    # Script principal para executar testes
â”œâ”€â”€ fixtures/
â”‚   â””â”€â”€ test_data.py                # Dados de teste reutilizÃ¡veis
â”œâ”€â”€ unit/                           # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ test_validation.py          # Testes para utils/validation.py
â”‚   â”œâ”€â”€ test_metrics.py             # Testes para utils/metrics.py
â”‚   â”œâ”€â”€ test_resource_manager.py    # Testes para utils/resource_manager.py
â”‚   â””â”€â”€ test_indexer.py             # Testes unitÃ¡rios para indexer.py
â”œâ”€â”€ integration/                    # Testes de integraÃ§Ã£o
â”‚   â””â”€â”€ test_indexer_integration.py # IntegraÃ§Ã£o do indexer com APIs externas
â””â”€â”€ functional/                     # Testes funcionais
    â”œâ”€â”€ test_search.py              # Testes do sistema de busca RAG
    â””â”€â”€ test_evaluator.py           # Testes do sistema de avaliaÃ§Ã£o
```

## ğŸ·ï¸ Categorias de Testes

### Testes UnitÃ¡rios (`unit/`)
- **Escopo**: Testam componentes individuais isoladamente
- **CaracterÃ­sticas**: RÃ¡pidos, sem dependÃªncias externas, mockam todas as dependÃªncias
- **Cobertura**: FunÃ§Ãµes utilitÃ¡rias, classes de configuraÃ§Ã£o, helpers

### Testes de IntegraÃ§Ã£o (`integration/`)
- **Escopo**: Testam integraÃ§Ã£o entre componentes do sistema
- **CaracterÃ­sticas**: Podem usar mocks para APIs externas, mas testam fluxos completos
- **Cobertura**: Pipelines de processamento, comunicaÃ§Ã£o entre mÃ³dulos

### Testes Funcionais (`functional/`)
- **Escopo**: Testam funcionalidades completas do sistema
- **CaracterÃ­sticas**: Simulam cenÃ¡rios reais de uso, podem ser mais lentos
- **Cobertura**: Sistema RAG completo, avaliaÃ§Ã£o end-to-end

## ğŸƒâ€â™‚ï¸ Como Executar os Testes

### Usando o script run_tests.py

```bash
# Executar todos os testes
python tests/run_tests.py all

# Executar apenas testes unitÃ¡rios
python tests/run_tests.py unit

# Executar testes de integraÃ§Ã£o
python tests/run_tests.py integration

# Executar testes funcionais
python tests/run_tests.py functional

# Executar testes rÃ¡pidos (exclui testes marcados como 'slow')
python tests/run_tests.py fast

# Executar testes de um mÃ³dulo especÃ­fico
python tests/run_tests.py module:indexer
python tests/run_tests.py module:validation

# Executar com anÃ¡lise de cobertura
python tests/run_tests.py coverage
```

### Usando pytest diretamente

```bash
# Todos os testes com verbose
pytest tests/ -v

# Apenas testes unitÃ¡rios
pytest tests/unit/ -m unit

# Excluir testes lentos
pytest tests/ -m "not slow"

# Com cobertura de cÃ³digo
pytest tests/ --cov=. --cov-report=html --cov-report=term-missing

# Testar arquivo especÃ­fico
pytest tests/unit/test_validation.py -v

# Executar teste especÃ­fico
pytest tests/unit/test_validation.py::TestValidateDocument::test_valid_document -v
```

## ğŸ¯ Marcadores (Marks)

Os testes usam marcadores para categorizaÃ§Ã£o:

- `@pytest.mark.unit` - Testes unitÃ¡rios
- `@pytest.mark.integration` - Testes de integraÃ§Ã£o  
- `@pytest.mark.functional` - Testes funcionais
- `@pytest.mark.slow` - Testes que demoram mais para executar
- `@pytest.mark.external` - Testes que dependem de recursos externos

## ğŸ”§ Fixtures Principais

### Fixtures Globais (conftest.py)
- `setup_test_environment` - Configura variÃ¡veis de ambiente para testes
- `temp_dir` - DiretÃ³rio temporÃ¡rio para testes
- `sample_document` - Documento de exemplo
- `sample_embedding` - Embedding de exemplo
- `mock_config` - ConfiguraÃ§Ã£o mockada
- `mock_voyage_client` - Cliente Voyage AI mockado
- `mock_astra_collection` - Collection Astra DB mockada
- `mock_openai_client` - Cliente OpenAI mockado

### Fixtures de Dados (fixtures/test_data.py)
- Perguntas de teste para avaliaÃ§Ã£o
- Documentos de exemplo
- Embeddings simulados
- Respostas RAG mockadas

## ğŸ“Š Cobertura de CÃ³digo

O projeto visa manter alta cobertura de cÃ³digo:

- **Alvo**: >90% de cobertura geral
- **MÃ³dulos crÃ­ticos**: >95% (validation, metrics, core indexer)
- **RelatÃ³rios**: HTML e terminal

### Verificar cobertura atual
```bash
python tests/run_tests.py coverage
```

Os relatÃ³rios serÃ£o gerados em:
- `htmlcov/index.html` - RelatÃ³rio HTML interativo
- Terminal - Resumo com linhas nÃ£o cobertas

## ğŸ§ª Boas PrÃ¡ticas Implementadas

### Isolamento
- Cada teste Ã© independente
- Uso extensivo de mocks para dependÃªncias externas
- Cleanup automÃ¡tico de recursos temporÃ¡rios

### Nomenclatura
- Nomes descritivos: `test_validate_document_missing_id`
- Prefixos indicam cenÃ¡rio: `test_`, `mock_`, `sample_`
- Classes agrupam testes relacionados

### ParametrizaÃ§Ã£o
- Testes parametrizados para mÃºltiplos cenÃ¡rios
- Reduz duplicaÃ§Ã£o de cÃ³digo
- Facilita adiÃ§Ã£o de novos casos

### Fixtures ReutilizÃ¡veis
- Dados de teste centralizados
- ConfiguraÃ§Ãµes padrÃ£o mockadas
- Facilita manutenÃ§Ã£o

## ğŸ› DepuraÃ§Ã£o de Testes

### Executar teste especÃ­fico com debug
```bash
pytest tests/unit/test_validation.py::TestValidateDocument::test_valid_document -v -s
```

### Ver output completo
```bash
pytest tests/ -v -s --tb=long
```

### Parar no primeiro erro
```bash
pytest tests/ -x
```

### Rodar apenas testes que falharam na Ãºltima execuÃ§Ã£o
```bash
pytest tests/ --lf
```

## ğŸ“ˆ MÃ©tricas de Teste

### Performance
- Testes unitÃ¡rios: < 1s total
- Testes de integraÃ§Ã£o: < 30s total
- Testes funcionais: < 2min total

### Qualidade
- Todos os testes devem passar
- Sem warnings desnecessÃ¡rios
- Coverage mÃ­nima mantida

## ğŸ”„ CI/CD (Futuro)

A estrutura estÃ¡ preparada para integraÃ§Ã£o contÃ­nua:

```yaml
# Exemplo para GitHub Actions
- name: Run Unit Tests
  run: python tests/run_tests.py unit

- name: Run Integration Tests  
  run: python tests/run_tests.py integration

- name: Check Coverage
  run: python tests/run_tests.py coverage
```

## ğŸ“ Adicionando Novos Testes

### Para novos mÃ³dulos:
1. Criar arquivo de teste correspondente em `unit/`
2. Adicionar testes de integraÃ§Ã£o em `integration/` se necessÃ¡rio
3. Atualizar `run_tests.py` com novo mÃ³dulo
4. Adicionar fixtures especÃ­ficas se necessÃ¡rio

### Para novos features:
1. Escrever testes primeiro (TDD)
2. Usar fixtures existentes quando possÃ­vel
3. Marcar apropriadamente com `@pytest.mark.*`
4. Documentar casos de edge testados

## ğŸ†˜ Troubleshooting

### Problema: ImportError
- Verificar se estÃ¡ executando do diretÃ³rio correto
- Verificar se `PYTHONPATH` inclui o diretÃ³rio do projeto

### Problema: Testes lentos
- Usar marcador `@pytest.mark.slow` para testes demorados
- Executar `python tests/run_tests.py fast` para pular testes lentos

### Problema: Falhas em mocks
- Verificar se mocks estÃ£o configurados corretamente
- Usar `patch` com paths absolutos
- Verificar ordem de decorators `@patch`

## ğŸ“š Recursos Adicionais

- [DocumentaÃ§Ã£o do pytest](https://docs.pytest.org/)
- [pytest-cov para cobertura](https://pytest-cov.readthedocs.io/)
- [Best practices para testes em Python](https://docs.python-guide.org/writing/tests/)
