# buscador_conversacional_producao.py

import os
import re
import base64
import json
import logging
from typing import List, Tuple, Optional, Dict, Any
from dotenv import load_dotenv

import voyageai
from openai import OpenAI
from PIL import Image
from astrapy import DataAPIClient

# Configura√ß√µes para produ√ß√£o
LLM_MODEL = "gpt-4o" 
MAX_CANDIDATES = 5
MAX_TOKENS_RERANK = 512
MAX_TOKENS_ANSWER = 2048
MAX_TOKENS_QUERY_TRANSFORM = 150  # Reduzido para efici√™ncia
COLLECTION_NAME = "pdf_documents"

# Logging estruturado para produ√ß√£o
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

class ProductionQueryTransformer:
    """
    Transformador de queries otimizado para produ√ß√£o
    - Menos chamadas LLM (economia de custos)
    - L√≥gica determin√≠stica quando poss√≠vel
    - Fallbacks robustos
    - Logging estruturado
    """
    
    def __init__(self, openai_client: OpenAI):
        self.openai_client = openai_client
        self.transformation_cache = {}  # Cache para transforma√ß√µes comuns
        self._init_patterns()
    
    def _init_patterns(self):
        """Inicializa padr√µes para classifica√ß√£o determin√≠stica"""
        self.greeting_patterns = {
            'simple': ['oi', 'ol√°', 'hello', 'hi', 'hey'],
            'formal': ['bom dia', 'boa tarde', 'boa noite', 'good morning'],
            'casual': ['opa', 'salve', 'e a√≠']
        }
        
        self.thank_patterns = [
            'obrigado', 'obrigada', 'thanks', 'thank you', 'valeu', 
            'brigado', 'grato', 'grata'
        ]
        
        self.document_terms = [
            'zep', 'graphiti', 'rag', 'temporal', 'knowledge graph',
            'grafo', 'arquitetura', 'paper', 'documento', 'artigo',
            'tabela', 'table', 'figura', 'figure', 'performance', 
            'resultado', 'metodologia', 'algorithm', 'invalida√ß√£o', 
            'mem√≥ria', 'embedding', 'vector', 'similarity'
        ]
        
        self.inquiry_keywords = [
            'explique', 'explain', 'como', 'how', 'o que', 'what',
            'quais', 'which', 'where', 'onde', 'quando', 'when',
            'por que', 'why', 'porque', 'me fale', 'tell me',
            'descreva', 'describe', 'mostre', 'show', 'qual',
            'quero saber', 'want to know', 'preciso entender',
            'pode explicar', 'can you explain'
        ]
        
        self.contextual_pronouns = [
            'isso', 'isto', 'aquilo', 'ele', 'ela', 'eles', 'elas',
            'this', 'that', 'these', 'those', 'it', 'they'
        ]
    
    def transform_query(self, chat_history: List[Dict[str, str]]) -> str:
        """
        Transforma√ß√£o principal com m√∫ltiplas estrat√©gias
        """
        try:
            if not chat_history:
                return "Not applicable"
            
            last_message = self._get_last_user_message(chat_history)
            if not last_message:
                return "Not applicable"
            
            # Cache para transforma√ß√µes j√° feitas
            cache_key = self._create_cache_key(last_message, chat_history)
            if cache_key in self.transformation_cache:
                logger.debug(f"Cache hit para: {last_message[:30]}...")
                return self.transformation_cache[cache_key]
            
            # 1. Verifica√ß√µes determin√≠sticas (sem LLM)
            deterministic_result = self._deterministic_classification(last_message, chat_history)
            if deterministic_result != "NEEDS_LLM":
                self.transformation_cache[cache_key] = deterministic_result
                return deterministic_result
            
            # 2. Transforma√ß√£o com LLM (apenas quando necess√°rio)
            llm_result = self._llm_transformation(last_message, chat_history)
            self.transformation_cache[cache_key] = llm_result
            
            return llm_result
            
        except Exception as e:
            logger.error(f"Erro na transforma√ß√£o de query: {e}")
            # Fallback seguro
            return self._safe_fallback(last_message)
    
    def _deterministic_classification(self, message: str, chat_history: List[Dict[str, str]]) -> str:
        """
        Classifica√ß√£o determin√≠stica sem LLM (mais r√°pida e barata)
        """
        message_lower = message.lower().strip()
        
        # 1. Sauda√ß√µes simples
        if self._is_simple_greeting(message_lower):
            return "Not applicable"
        
        # 2. Agradecimentos simples
        if self._is_simple_thanks(message_lower):
            return "Not applicable"
        
        # 3. Men√ß√µes diretas ao documento
        if self._mentions_document_directly(message_lower):
            return message  # J√° est√° bem formada
        
        # 4. Perguntas gerais sobre o documento
        if self._is_general_document_inquiry(message_lower):
            return f"Sobre o documento Zep: {message}"
        
        # 5. Refer√™ncias contextuais (pronomes)
        if self._has_contextual_references(message_lower) and self._has_document_context(chat_history):
            return f"Sobre o Zep: {message}"
        
        # 6. Perguntas com palavras-chave de consulta
        if self._has_inquiry_pattern(message_lower):
            return f"Sobre o documento Zep: {message}"
        
        # Se chegou aqui, precisa de LLM para contexto mais complexo
        return "NEEDS_LLM"
    
    def _is_simple_greeting(self, message: str) -> bool:
        """Detecta sauda√ß√µes simples"""
        words = message.split()
        
        # Sauda√ß√µes de 1-2 palavras
        if len(words) <= 2:
            for pattern_group in self.greeting_patterns.values():
                if all(word in pattern_group for word in words):
                    return True
        
        return False
    
    def _is_simple_thanks(self, message: str) -> bool:
        """Detecta agradecimentos simples"""
        return any(thank in message for thank in self.thank_patterns) and len(message.split()) <= 3
    
    def _mentions_document_directly(self, message: str) -> bool:
        """Verifica se menciona termos do documento diretamente"""
        return any(term in message for term in self.document_terms)
    
    def _is_general_document_inquiry(self, message: str) -> bool:
        """Detecta perguntas gerais que precisam de contexto do documento"""
        has_inquiry = any(keyword in message for keyword in self.inquiry_keywords)
        is_question = any(char in message for char in ['?', 'qual', 'como', 'o que'])
        
        return has_inquiry or is_question
    
    def _has_contextual_references(self, message: str) -> bool:
        """Detecta pronomes que referenciam contexto anterior"""
        return any(pronoun in message for pronoun in self.contextual_pronouns)
    
    def _has_document_context(self, chat_history: List[Dict[str, str]]) -> bool:
        """Verifica se h√° contexto do documento nas mensagens recentes"""
        recent_messages = chat_history[-6:]  # √öltimas 6 mensagens
        
        for msg in recent_messages:
            if msg.get('role') == 'assistant':
                content = msg.get('content', '').lower()
                if any(term in content for term in ['zep', 'graphiti', 'documento', 'p√°gina']):
                    return True
        
        return False
    
    def _has_inquiry_pattern(self, message: str) -> bool:
        """Detecta padr√µes de consulta"""
        return any(keyword in message for keyword in self.inquiry_keywords)
    
    def _llm_transformation(self, message: str, chat_history: List[Dict[str, str]]) -> str:
        """
        Transforma√ß√£o com LLM - usada apenas quando necess√°rio
        """
        try:
            # Contexto reduzido para economizar tokens
            recent_context = self._build_minimal_context(chat_history[-4:])
            
            prompt = f"""Transforme a mensagem em uma pergunta espec√≠fica sobre documentos acad√™micos.

REGRAS:
1. Se menciona "Zep", mantenha como est√°
2. Se √© pergunta geral, adicione "Sobre o Zep:"
3. Se referencia conversa anterior, combine contextos
4. Seja conciso e direto

CONTEXTO RECENTE:
{recent_context}

MENSAGEM: {message}

RESPONDA APENAS COM A PERGUNTA TRANSFORMADA:"""

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=MAX_TOKENS_QUERY_TRANSFORM,
                temperature=0.0
            )
            
            transformed = response.choices[0].message.content.strip()
            
            # Limpeza p√≥s-LLM
            transformed = self._clean_llm_output(transformed)
            
            logger.debug(f"LLM transform: '{message}' ‚Üí '{transformed}'")
            return transformed
            
        except Exception as e:
            logger.error(f"Erro na transforma√ß√£o LLM: {e}")
            return self._safe_fallback(message)
    
    def _build_minimal_context(self, recent_messages: List[Dict[str, str]]) -> str:
        """Constr√≥i contexto m√≠nimo para economizar tokens"""
        context_parts = []
        
        for msg in recent_messages:
            role = msg.get('role', '')
            content = msg.get('content', '')[:100]  # Limita a 100 chars
            
            if role in ['user', 'assistant']:
                context_parts.append(f"{role.title()}: {content}")
        
        return "\n".join(context_parts)
    
    def _clean_llm_output(self, output: str) -> str:
        """Limpa sa√≠da do LLM"""
        # Remove prefixos comuns
        prefixes = ['rag query:', 'query:', 'pergunta:', 'question:']
        
        for prefix in prefixes:
            if output.lower().startswith(prefix):
                output = output[len(prefix):].strip()
        
        # Remove aspas extras
        output = output.strip('"\'')
        
        return output
    
    def _safe_fallback(self, message: str) -> str:
        """Fallback seguro quando tudo mais falha"""
        if 'zep' in message.lower():
            return message
        else:
            return f"Sobre o documento Zep: {message}"
    
    def _create_cache_key(self, message: str, chat_history: List[Dict[str, str]]) -> str:
        """Cria chave de cache baseada na mensagem e contexto"""
        # Contexto simplificado para cache
        recent_topics = []
        for msg in chat_history[-3:]:
            content = msg.get('content', '').lower()
            if 'zep' in content:
                recent_topics.append('zep')
            if 'graphiti' in content:
                recent_topics.append('graphiti')
        
        context_key = '+'.join(set(recent_topics))
        return f"{message.lower()[:50]}||{context_key}"
    
    def _get_last_user_message(self, chat_history: List[Dict[str, str]]) -> str:
        """Pega √∫ltima mensagem do usu√°rio"""
        for msg in reversed(chat_history):
            if msg.get("role") == "user":
                return msg.get("content", "")
        return ""
    
    def needs_rag(self, transformed_query: str) -> bool:
        """Verifica se precisa fazer RAG"""
        return "not applicable" not in transformed_query.lower()
    
    def clean_query(self, transformed_query: str) -> str:
        """Limpa query final"""
        return transformed_query.strip()
    
    def get_cache_stats(self) -> Dict[str, int]:
        """Estat√≠sticas do cache para monitoramento"""
        return {
            "cache_size": len(self.transformation_cache),
            "cache_hits": getattr(self, '_cache_hits', 0),
            "llm_calls": getattr(self, '_llm_calls', 0)
        }

class ProductionConversationalRAG:
    """
    Sistema RAG conversacional otimizado para produ√ß√£o
    """
    
    def __init__(self) -> None:
        """Inicializa com configura√ß√µes de produ√ß√£o"""
        load_dotenv()

        # Valida√ß√£o de ambiente
        required_vars = [
            "VOYAGE_API_KEY", "OPENAI_API_KEY",
            "ASTRA_DB_API_ENDPOINT", "ASTRA_DB_APPLICATION_TOKEN"
        ]
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        if missing_vars:
            raise ValueError(f"Vari√°veis de ambiente ausentes: {missing_vars}")

        # Inicializa√ß√£o dos clientes
        voyageai.api_key = os.environ["VOYAGE_API_KEY"]
        self.voyage_client = voyageai.Client()
        self.openai_client = OpenAI()
        
        # Transformador otimizado para produ√ß√£o
        self.query_transformer = ProductionQueryTransformer(self.openai_client)
        
        # Hist√≥rico da conversa
        self.chat_history: List[Dict[str, str]] = []

        # Conex√£o com Astra DB
        self._initialize_database()
        
        logger.info("Sistema RAG de produ√ß√£o inicializado com sucesso")

    def _initialize_database(self):
        """Inicializa conex√£o com base de dados"""
        try:
            logger.info("Conectando ao Astra DB...")
            client = DataAPIClient()
            database = client.get_database(
                os.environ["ASTRA_DB_API_ENDPOINT"], 
                token=os.environ["ASTRA_DB_APPLICATION_TOKEN"]
            )
            self.collection = database.get_collection(COLLECTION_NAME)
            
            # Teste de conectividade
            list(self.collection.find({}, limit=1))
            logger.info(f"Conectado ao Astra DB - Collection '{COLLECTION_NAME}' acess√≠vel")
                
        except Exception as e:
            logger.error(f"Falha ao conectar Astra DB: {e}")
            raise

    def ask(self, user_message: str) -> str:
        """Interface conversacional principal otimizada"""
        try:
            # Adiciona mensagem do usu√°rio ao hist√≥rico
            self.chat_history.append({"role": "user", "content": user_message})
            
            # Transforma em query RAG
            transformed_query = self.query_transformer.transform_query(self.chat_history)
            
            # Verifica se precisa fazer RAG
            if not self.query_transformer.needs_rag(transformed_query):
                response = self._generate_non_rag_response(user_message)
            else:
                # Limpa a query e faz RAG
                clean_query = self.query_transformer.clean_query(transformed_query)
                logger.info(f"Fazendo RAG para: '{clean_query}'")
                
                rag_result = self.search_and_answer(clean_query)
                
                if "error" in rag_result:
                    response = f"Desculpe, n√£o consegui encontrar informa√ß√µes sobre isso. {rag_result['error']}"
                else:
                    response = rag_result["answer"]
            
            # Adiciona resposta ao hist√≥rico
            self.chat_history.append({"role": "assistant", "content": response})
            
            # Limita hist√≥rico para controle de mem√≥ria
            if len(self.chat_history) > 20:
                self.chat_history = self.chat_history[-16:]
            
            return response
            
        except Exception as e:
            logger.error(f"Erro no processamento: {e}")
            return "Desculpe, ocorreu um erro interno. Tente novamente."

    def _generate_non_rag_response(self, user_message: str) -> str:
        """Gera resposta para mensagens que n√£o precisam de RAG"""
        greetings = ["oi", "ol√°", "hello", "hi", "boa tarde", "bom dia", "boa noite"]
        
        if any(greeting in user_message.lower() for greeting in greetings):
            return "Ol√°! Sou seu assistente para consultas sobre documentos acad√™micos. Como posso ajudar voc√™ hoje?"
        
        thanks = ["obrigado", "obrigada", "thanks", "valeu"]
        if any(thank in user_message.lower() for thank in thanks):
            return "De nada! Fico feliz em ajudar. H√° mais alguma coisa que gostaria de saber?"
        
        return "Como posso ajudar voc√™ com consultas sobre os documentos? Fa√ßa uma pergunta espec√≠fica e eu buscarei as informa√ß√µes relevantes."

    # M√©todos de RAG originais (mantidos para compatibilidade)
    def get_query_embedding(self, query: str) -> List[float]:
        """Gera embedding para a consulta"""
        try:
            res = self.voyage_client.multimodal_embed(
                inputs=[[query]],
                model="voyage-multimodal-3",
                input_type="query"
            )
            return res.embeddings[0]
        except Exception as e:
            logger.error(f"Erro embedding consulta: {e}")
            raise

    @staticmethod
    def encode_image_to_base64(image_path: str) -> Optional[str]:
        """Converte imagem local em base64"""
        try:
            if not image_path or not os.path.exists(image_path):
                logger.warning(f"Imagem n√£o encontrada: {image_path}")
                return None
            with open(image_path, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            logger.error(f"Erro codificando {image_path}: {e}")
            return None

    def search_candidates(self, query_embedding: List[float], limit: int = MAX_CANDIDATES) -> List[dict]:
        """Busca candidatos no Astra DB"""
        try:
            logger.debug("Buscando similaridade no Astra DB...")
            
            cursor = self.collection.find(
                {},
                sort={"$vector": query_embedding},
                limit=limit,
                include_similarity=True,
                projection={
                    "file_path": True,
                    "page_num": True,
                    "doc_source": True,
                    "markdown_text": True,
                    "_id": True
                }
            )
            
            candidates = []
            for doc in cursor:
                candidates.append({
                    "file_path": doc.get("file_path"),
                    "page_num": doc.get("page_num"),
                    "doc_source": doc.get("doc_source"),
                    "markdown_text": doc.get("markdown_text", ""),
                    "similarity_score": doc.get("$similarity", 0.0),
                })
            
            logger.info(f"Busca retornou {len(candidates)} candidatos")
            return candidates
        except Exception as e:
            logger.error(f"Erro busca Astra DB: {e}")
            return []

    def verify_relevance(self, query: str, selected: List[dict]) -> bool:
        """Verifica relev√¢ncia do contexto selecionado"""
        if not selected:
            return False

        try:
            logger.debug("Verificando relev√¢ncia do contexto...")
            context_text = "\n\n".join(
                f"=== P√ÅGINA {c['page_num']} ===\n{c['markdown_text']}"
                for c in selected
            )

            prompt = (
                f"Analise o conte√∫do para responder: \"{query}\"\n\n"
                f"Conte√∫do:\n---\n{context_text}\n---\n\n"
                "O conte√∫do cont√©m resposta factual para a pergunta? "
                "Responda apenas 'Sim' ou 'N√£o'."
            )

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=5,
                temperature=0.0
            )
            
            verification_result = response.choices[0].message.content or ""
            logger.debug(f"Verifica√ß√£o de relev√¢ncia: '{verification_result}'")
            
            return "sim" in verification_result.lower()

        except Exception as e:
            logger.error(f"Erro na verifica√ß√£o de relev√¢ncia: {e}")
            return True  # Fallback conservador

    def rerank_with_gpt(self, query: str, candidates: List[dict]) -> Tuple[List[dict], str]:
        """Re-ranking com GPT-4"""
        if not candidates:
            return [], "Nenhuma p√°gina dispon√≠vel."

        if len(candidates) == 1:
            c = candidates[0]
            doc_name = os.path.basename(c["file_path"]).replace(".png", "")
            return [c], f"√önica p√°gina {doc_name}, p.{c['page_num']}."

        try:
            pages_info = ", ".join(
                f"{os.path.basename(c['file_path']).replace('.png','')} (p.{c['page_num']})"
                for c in candidates
            )
            
            prompt_head = (
                f"Pergunta: '{query}'.\n"
                f"P√°ginas ({len(candidates)}): {pages_info}.\n"
                "Selecione apenas a p√°gina mais relevante. "
                "M√°ximo 2 p√°ginas se absolutamente necess√°rio.\n\n"
                "Formato:\n"
                "P√°ginas_Selecionadas: [n¬∫] ou [n¬∫1, n¬∫2]\n"
                "Justificativa: ‚Ä¶"
            )
            content = [{"type": "text", "text": prompt_head}]

            for cand in candidates:
                b64 = self.encode_image_to_base64(cand["file_path"])
                if not b64:
                    continue
                    
                preview = cand["markdown_text"][:300]
                text_block = (
                    f"\n=== {os.path.basename(cand['file_path']).replace('.png','').upper()} "
                    f"- P√ÅGINA {cand['page_num']} ===\n"
                    f"Score: {cand['similarity_score']:.4f}\n"
                    f"Texto: {preview}{'‚Ä¶' if len(cand['markdown_text'])>300 else ''}\n"
                )
                content.append({"type": "text", "text": text_block})
                content.append({"type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{b64}"}})

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_RERANK,
                temperature=0.0
            )
            
            result = response.choices[0].message.content or ""
            logger.debug(f"Re-ranker: {result}")

            # Parse da resposta
            selected_nums: List[int] = []
            justification = "Justificativa ausente."
            
            for line in result.splitlines():
                if line.lower().startswith("p√°ginas_selecionadas"):
                    selected_nums = [int(n) for n in re.findall(r"\d+", line)]
                elif line.startswith("Justificativa:"):
                    justification = line.replace("Justificativa:", "").strip()

            chosen = [c for c in candidates if c["page_num"] in selected_nums]
            if chosen:
                return chosen, justification
                
            logger.warning("Re-ranker n√£o selecionou p√°ginas v√°lidas")
            return [candidates[0]], "Fallback: usando candidato mais similar."

        except Exception as e:
            logger.error(f"Erro re-ranking: {e}")
            return [candidates[0]], "Fallback: erro no re-ranker."

    def generate_conversational_answer(self, query: str, selected: List[dict]) -> str:
        """Gera resposta conversacional otimizada"""
        try:
            no_md = "N√ÉO use formata√ß√£o Markdown como **, _, #. Escreva texto corrido."
            
            if len(selected) == 1:
                c = selected[0]
                doc = os.path.basename(c["file_path"]).split("_page_")[0]
                
                prompt = (
                    f"Assistente especializado em documentos acad√™micos.\n"
                    f"Pergunta: {query}\n\n"
                    f"Use APENAS a p√°gina {c['page_num']} do documento '{doc}'.\n"
                    f"Texto da p√°gina:\n{c['markdown_text']}\n\n"
                    f"Instru√ß√µes: resposta clara e direta. Cite: documento '{doc}', p√°gina {c['page_num']}.\n"
                    f"{no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                b64 = self.encode_image_to_base64(c["file_path"])
                if b64:
                    content.append({"type": "image_url",
                                    "image_url": {"url": f"data:image/png;base64,{b64}"}})

            else:
                pages_str = " e ".join(
                    f"{os.path.basename(c['file_path']).split('_page_')[0]} p.{c['page_num']}"
                    for c in selected
                )
                combined_text = "\n\n".join(
                    f"=== P√ÅGINA {c['page_num']} ===\n{c['markdown_text']}"
                    for c in selected
                )
                
                prompt = (
                    f"Pergunta: {query}\n\n"
                    f"Use p√°ginas: {pages_str}\n"
                    f"{combined_text}\n\n"
                    f"Integre informa√ß√µes. Cite fontes. {no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                for c in selected:
                    b64 = self.encode_image_to_base64(c["file_path"])
                    if b64:
                        content.append({"type": "text", "text": f"\n--- P√ÅGINA {c['page_num']} ---"})
                        content.append({"type": "image_url",
                                        "image_url": {"url": f"data:image/png;base64,{b64}"}})

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_ANSWER,
                temperature=0.2
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Erro gerando resposta: {e}")
            return f"Erro ao processar resposta: {e}"

    def search_and_answer(self, query: str) -> dict:
        """Pipeline completo RAG"""
        logger.info(f"Consulta: '{query}'")
        
        try:
            embedding = self.get_query_embedding(query)
        except Exception as e:
            return {"error": f"Embedding falhou: {e}"}

        candidates = self.search_candidates(embedding)
        if not candidates:
            return {"error": "Nenhuma p√°gina relevante encontrada."}

        logger.debug("Re-rankeando candidatos...")
        selected, justification = self.rerank_with_gpt(query, candidates)
        if not selected:
            return {"error": "Re-ranking falhou."}

        if not self.verify_relevance(query, selected):
            logger.warning("Verifica√ß√£o de relev√¢ncia indicou baixa relev√¢ncia")
            return {
                "error": "A informa√ß√£o solicitada n√£o foi encontrada de forma expl√≠cita no documento."
            }

        logger.debug("Gerando resposta final...")
        answer = self.generate_conversational_answer(query, selected)

        # Prepara detalhes da resposta
        sel_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in selected
        ]
        
        all_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in candidates
        ]
        
        sel_str = " + ".join(
            f"{p['document']} (p.{p['page_number']})" for p in sel_details
        )

        return {
            "query": query,
            "selected_pages": sel_str,
            "selected_pages_details": sel_details,
            "selected_pages_count": len(selected),
            "justification": justification,
            "answer": answer,
            "total_candidates": len(candidates),
            "all_candidates": all_details,
        }

    def extract_structured_data(self, template: dict, document_filter: Optional[str] = None) -> dict:
        """Extra√ß√£o de dados estruturados"""
        try:
            # Busca p√°ginas relevantes
            if document_filter:
                pages_cursor = self.collection.find(
                    {"doc_source": document_filter},
                    limit=10,
                    projection={
                        "file_path": True,
                        "page_num": True,
                        "doc_source": True,
                        "markdown_text": True
                    }
                )
            else:
                pages_cursor = self.collection.find(
                    {},
                    limit=10,
                    projection={
                        "file_path": True,
                        "page_num": True,
                        "doc_source": True,
                        "markdown_text": True
                    }
                )
            
            pages = list(pages_cursor)
            if not pages:
                return {"error": "Nenhuma p√°gina encontrada"}
            
            # Prepara prompt de extra√ß√£o
            template_str = json.dumps(template, indent=2)
            
            content = [{
                "type": "text",
                "text": f"""
Extraia dados estruturados seguindo este template: {template_str}

Se informa√ß√£o n√£o dispon√≠vel, deixe em branco.
Responda APENAS com JSON v√°lido.

DOCUMENTOS:"""
            }]
            
            # Adiciona p√°ginas (limitado para n√£o exceder tokens)
            for page in pages[:5]:
                doc_name = page.get("doc_source", "documento")
                page_num = page.get("page_num", 0)
                content_text = page.get("markdown_text", "")[:500]
                
                content.append({
                    "type": "text",
                    "text": f"\n=== {doc_name.upper()} - P√ÅGINA {page_num} ===\n{content_text}\n"
                })
                
                # Adiciona imagem se dispon√≠vel
                img_b64 = self.encode_image_to_base64(page.get("file_path"))
                if img_b64:
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{img_b64}"}
                    })
            
            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            extracted_data = json.loads(response.choices[0].message.content)
            
            return {
                "status": "success",
                "data": extracted_data,
                "pages_analyzed": len(pages)
            }
            
        except Exception as e:
            logger.error(f"Erro na extra√ß√£o de dados: {e}")
            return {
                "status": "error",
                "message": f"Erro na extra√ß√£o: {e}"
            }

    def clear_history(self):
        """Limpa hist√≥rico da conversa"""
        self.chat_history = []
        logger.info("Hist√≥rico de conversa limpo")

    def get_chat_history(self) -> List[Dict[str, str]]:
        """Retorna hist√≥rico atual"""
        return self.chat_history.copy()

    def get_system_stats(self) -> Dict[str, Any]:
        """Estat√≠sticas do sistema para monitoramento"""
        stats = {
            "chat_history_length": len(self.chat_history),
            "transformer_stats": self.query_transformer.get_cache_stats(),
            "system_health": "operational"
        }
        
        try:
            # Teste de conectividade
            list(self.collection.find({}, limit=1))
            stats["database_status"] = "connected"
        except:
            stats["database_status"] = "error"
            stats["system_health"] = "degraded"
        
        return stats

# Wrapper para compatibilidade com c√≥digo existente
class ConversationalMultimodalRAG(ProductionConversationalRAG):
    """Alias para compatibilidade com c√≥digo existente"""
    pass

# Classe simples para interface externa
class SimpleRAG:
    """Interface simplificada para uso externo"""
    
    def __init__(self):
        self.rag = ProductionConversationalRAG()
    
    def search(self, query: str) -> str:
        """Busca simples"""
        return self.rag.ask(query)
    
    def extract(self, template: dict, document: str = None) -> dict:
        """Extrai dados estruturados"""
        return self.rag.extract_structured_data(template, document)
    
    def clear_chat(self):
        """Limpa hist√≥rico"""
        self.rag.clear_history()

# Interface CLI otimizada para produ√ß√£o
def main() -> None:
    """Interface CLI com tratamento de erros robusto"""
    try:
        rag = ProductionConversationalRAG()
        
        print("üöÄ SISTEMA RAG CONVERSACIONAL - PRODU√á√ÉO üöÄ")
        print("=" * 70)
        print("‚ú® Otimiza√ß√µes de produ√ß√£o ativas:")
        print("  ‚Ä¢ Query transformer inteligente")
        print("  ‚Ä¢ Cache de transforma√ß√µes")
        print("  ‚Ä¢ Logging estruturado")
        print("  ‚Ä¢ Fallbacks robustos")
        print("=" * 70)

        print(rag.ask("Ol√°!"))
        print()

        while True:
            try:
                user_input = input("üí¨ Voc√™: ").strip()
                
                if user_input.lower() in {"sair", "exit", "quit", "/quit"}:
                    print("üëã At√© logo!")
                    break
                
                if not user_input:
                    continue
                
                # Comandos especiais
                if user_input.startswith("/"):
                    if user_input == "/help":
                        print_production_help()
                    elif user_input == "/clear":
                        rag.clear_history()
                        print("üßπ Hist√≥rico limpo!")
                    elif user_input == "/stats":
                        stats = rag.get_system_stats()
                        print("üìä Estat√≠sticas do sistema:")
                        for key, value in stats.items():
                            print(f"  {key}: {value}")
                    elif user_input.startswith("/extract"):
                        handle_production_extract_command(rag, user_input)
                    else:
                        print("‚ùì Comando n√£o reconhecido. Digite /help")
                    continue

                # Resposta normal
                print("\nü§ñ Assistente: ", end="")
                try:
                    response = rag.ask(user_input)
                    print(response)
                except Exception as e:
                    logger.error(f"Erro no processamento: {e}")
                    print("‚ùå Erro tempor√°rio. Tente novamente.")
                print()

            except KeyboardInterrupt:
                print("\nüëã At√© logo!")
                break
            except Exception as e:
                logger.error(f"Erro na interface: {e}")
                print("‚ùå Erro na interface. Continuando...")

    except Exception as e:
        logger.critical(f"Erro fatal: {e}")
        print(f"‚ùå Erro fatal na inicializa√ß√£o: {e}")
        print("Verifique:")
        print("1. Arquivo .env com chaves corretas")
        print("2. Conex√£o com Astra DB")
        print("3. Documentos indexados")

def print_production_help():
    """Ajuda para vers√£o de produ√ß√£o"""
    print("""
üìö COMANDOS DE PRODU√á√ÉO:
‚Ä¢ /help     - Esta ajuda
‚Ä¢ /clear    - Limpa hist√≥rico
‚Ä¢ /stats    - Estat√≠sticas do sistema
‚Ä¢ /extract  - Extra√ß√£o de dados
  Exemplo: /extract {"title": "", "authors": []}

üí° RECURSOS DE PRODU√á√ÉO:
‚Ä¢ Cache de transforma√ß√µes (economia de custos)
‚Ä¢ Fallbacks autom√°ticos (maior robustez)
‚Ä¢ Logging estruturado (monitoramento)
‚Ä¢ Valida√ß√£o de ambiente (seguran√ßa)

üîç TIPOS DE CONSULTA OTIMIZADOS:
‚Ä¢ Perguntas diretas sobre o Zep
‚Ä¢ Refer√™ncias contextuais ("como funciona isso?")
‚Ä¢ Consultas t√©cnicas espec√≠ficas
‚Ä¢ Seguimento de conversas anteriores
""")

def handle_production_extract_command(rag, command):
    """Manipula extra√ß√£o de dados na vers√£o de produ√ß√£o"""
    try:
        if len(command.split(" ", 1)) < 2:
            print("üí° Uso: /extract {\"campo\": \"valor\"}")
            print("üìù Exemplo: /extract {\"title\": \"\", \"methodology\": \"\"}")
            return
        
        template_str = command.split(" ", 1)[1]
        template = json.loads(template_str)
        
        print("üîç Extraindo dados (produ√ß√£o)...")
        result = rag.extract_structured_data(template)
        
        if result.get("status") == "success":
            print("‚úÖ Extra√ß√£o bem-sucedida:")
            print(json.dumps(result["data"], indent=2, ensure_ascii=False))
            print(f"üìä P√°ginas analisadas: {result['pages_analyzed']}")
        else:
            print(f"‚ùå Erro: {result.get('message')}")
            
    except json.JSONDecodeError:
        print("‚ùå JSON inv√°lido. Use aspas duplas!")
    except Exception as e:
        logger.error(f"Erro na extra√ß√£o: {e}")
        print(f"‚ùå Erro: {e}")

# Para monitoramento em produ√ß√£o
def health_check() -> Dict[str, str]:
    """Health check para monitoramento"""
    try:
        rag = ProductionConversationalRAG()
        stats = rag.get_system_stats()
        
        return {
            "status": "healthy" if stats["system_health"] == "operational" else "degraded",
            "database": stats["database_status"],
            "cache_size": str(stats["transformer_stats"]["cache_size"]),
            "timestamp": str(datetime.now())
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "timestamp": str(datetime.now())
        }

# Aliases para compatibilidade
MultimodalRagSearcher = ProductionConversationalRAG  # Para avaliador
EnhancedMultimodalRagSearcher = ProductionConversationalRAG  # Para sistemas melhorados

__all__ = [
    'ProductionConversationalRAG', 
    'ConversationalMultimodalRAG',
    'SimpleRAG', 
    'MultimodalRagSearcher',
    'health_check'
]

if __name__ == "__main__":
    main()